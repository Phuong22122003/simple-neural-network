{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200b69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\Python1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from models import Model_Adam\n",
    "from layers import Embedding,PositionalEncoding,TransformerBlock,Dense,LayerNorm,softmax\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f9f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8a8c197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(r\"D:\\Workspace\\Python\\Project\\NeuralNetwork\\dataset\\tokenizer\\tokenizer_vi.json\")\n",
    "\n",
    "dataset = load_dataset(\"justinphan3110/vi_pubmed\", split=\"vi\", streaming=True)\n",
    "dataset_iter = iter(islice(dataset, 64*3000))\n",
    "pad_token = '<|pad|>'\n",
    "maxlen = 64\n",
    "pad_id = tokenizer.token_to_id(pad_token)\n",
    "def split_token_batch(texts):\n",
    "        x, y = [], []\n",
    "    \n",
    "        # batch encode\n",
    "        encodings = tokenizer.encode_batch(texts)\n",
    "    \n",
    "        for encoding in encodings:\n",
    "            token = encoding.ids\n",
    "    \n",
    "            if len(token) <= maxlen + 1:\n",
    "                token = token + (maxlen+1-len(token))*[pad_id]\n",
    "                x.append(token[:-1])\n",
    "                y.append(token[1:])\n",
    "                \n",
    "    \n",
    "            else:\n",
    "                x.append(token[:maxlen])\n",
    "                y.append(token[1:maxlen + 1])\n",
    "        return x, y\n",
    "def data_generator(texts):\n",
    "    X, Y = split_token_batch([str(t['text']) for t in texts])\n",
    "    return np.array(X, dtype=np.int32), np.array(Y, dtype=np.int32)\n",
    "\n",
    "\n",
    "# batch_texts = list(islice(dataset_iter, 64*3))\n",
    "# X,y = data_generator(batch_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0659964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(batch_size, seq_len):\n",
    "    \"\"\"Create causal mask for autoregressive generation\"\"\"\n",
    "    mask = np.ones((batch_size, seq_len, seq_len))\n",
    "    return np.tril(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f61afb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyGPTModel(Model_Adam):\n",
    "    \"\"\"GPT-style autoregressive transformer model\"\"\"\n",
    "    def __init__(self, vocab_size=10000, max_len=256, embedding_dim=768,\n",
    "                 n_heads=12, n_layers=12):\n",
    "        pad_token = '<|pad|>'\n",
    "        pad_id = tokenizer.token_to_id(pad_token)\n",
    "        super().__init__()\n",
    "        # super().__init__(vocab_size=vocab_size, row_num=1_000_000, maxlen=max_len, pad_id = pad_id)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_ff = 4 * embedding_dim  \n",
    "        \n",
    "        self.token_embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_len, embedding_dim)\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embedding_dim, n_heads, self.d_ff)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        \n",
    "        self.ln_f = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.lm_head = Dense(vocab_size, activation='softmax')\n",
    "        # self.lm_head = Dense_Attention(vocab_size, activation='softmax')\n",
    "    def call(self, input_ids,training=False):\n",
    "        batch_size, seq_len = input_ids.shape       \n",
    "\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.pos_encoding(x,training=False)\n",
    "        \n",
    "        causal_mask = create_causal_mask(batch_size, seq_len)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block.forward(x, mask=causal_mask)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # logits = self.lm_head(x)\n",
    "        # print(training)\n",
    "        if training:\n",
    "            logits = self.lm_head(x)           # (batch, seq_len, vocab_size)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, -1:, :]) # (batch, vocab_size) - last token\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _all_layer(self):\n",
    "        \"\"\"Override to collect all layers including transformer blocks\"\"\"\n",
    "        layers = []\n",
    "\n",
    "        layers.append(self.token_embedding)\n",
    "        layers.extend(self.transformer_blocks)\n",
    "        layers.append(self.ln_f)\n",
    "        layers.append(self.lm_head)\n",
    "        \n",
    "        return layers\n",
    "\n",
    "model = MyGPTModel(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=128,      \n",
    "    max_len=64,            \n",
    "    n_heads=4,                 \n",
    "    n_layers=1,\n",
    ")\n",
    "\n",
    "# model.fit(X, y, learning_rate=1e-3, epochs=1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "621bb5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1081/192000 [05:26<16:01:17,  3.31it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 18\u001b[39m\n\u001b[32m     15\u001b[39m X = np.array(x_tokens, dtype=np.int32).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     16\u001b[39m y = np.array(y_tokens, dtype=np.int32).reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\Project\\NeuralNetwork\\models\\model_with_adam.py:77\u001b[39m, in \u001b[36mModel_Adam.fit\u001b[39m\u001b[34m(self, inputs, outputs, epochs, batch_size, learning_rate, beta1, beta2, epsilon, verbose)\u001b[39m\n\u001b[32m     74\u001b[39m outputs_batch = \u001b[38;5;28mself\u001b[39m.to_one_hot(outputs[i:i+batch_size, :], \u001b[38;5;28mself\u001b[39m.layers[-\u001b[32m1\u001b[39m].output_dim)\n\u001b[32m     76\u001b[39m \u001b[38;5;66;03m# Forward pass\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m pred_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcall\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     78\u001b[39m loss_val = cross_entropy(outputs_batch, pred_outputs)\n\u001b[32m     80\u001b[39m total_loss += loss_val\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 36\u001b[39m, in \u001b[36mMyGPTModel.call\u001b[39m\u001b[34m(self, input_ids, training)\u001b[39m\n\u001b[32m     33\u001b[39m causal_mask = create_causal_mask(batch_size, seq_len)\n\u001b[32m     35\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer_blocks:\n\u001b[32m---> \u001b[39m\u001b[32m36\u001b[39m     x = \u001b[43mblock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcausal_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     38\u001b[39m x = \u001b[38;5;28mself\u001b[39m.ln_f(x)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# logits = self.lm_head(x)\u001b[39;00m\n\u001b[32m     41\u001b[39m \u001b[38;5;66;03m# print(training)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\Project\\NeuralNetwork\\layers\\transfomer.py:52\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, mask)\u001b[39m\n\u001b[32m     49\u001b[39m \u001b[38;5;28mself\u001b[39m.ln1_output_cache = ln1_output.copy()\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 2. Multi-head attention\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m attn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mln1_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     53\u001b[39m \u001b[38;5;28mself\u001b[39m.attn_output_cache = attn_output.copy()\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# 3. Residual connection cho attention\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\Project\\NeuralNetwork\\layers\\attention.py:27\u001b[39m, in \u001b[36mAttention.forward\u001b[39m\u001b[34m(self, inputs, mask)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     25\u001b[39m     score = np.where(mask==\u001b[32m0\u001b[39m, -\u001b[32m1e9\u001b[39m, score)\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m score = \u001b[43msoftmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m \u001b[38;5;28mself\u001b[39m.mask = mask\n\u001b[32m     30\u001b[39m \u001b[38;5;28mself\u001b[39m.score = score\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\Project\\NeuralNetwork\\layers\\activation.py:22\u001b[39m, in \u001b[36msoftmax\u001b[39m\u001b[34m(Z)\u001b[39m\n\u001b[32m     21\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msoftmax\u001b[39m(Z):\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m     expZ = np.exp(Z - \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m) \n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m expZ / np.sum(expZ, axis=-\u001b[32m1\u001b[39m, keepdims=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\App\\Python1\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:2899\u001b[39m, in \u001b[36mmax\u001b[39m\u001b[34m(a, axis, out, keepdims, initial, where)\u001b[39m\n\u001b[32m   2781\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_max_dispatcher)\n\u001b[32m   2782\u001b[39m \u001b[38;5;129m@set_module\u001b[39m(\u001b[33m'\u001b[39m\u001b[33mnumpy\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mmax\u001b[39m(a, axis=\u001b[38;5;28;01mNone\u001b[39;00m, out=\u001b[38;5;28;01mNone\u001b[39;00m, keepdims=np._NoValue, initial=np._NoValue,\n\u001b[32m   2784\u001b[39m          where=np._NoValue):\n\u001b[32m   2785\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[33;03m    Return the maximum of an array or maximum along an axis.\u001b[39;00m\n\u001b[32m   2787\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2897\u001b[39m \u001b[33;03m    5\u001b[39;00m\n\u001b[32m   2898\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2899\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapreduction\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmaximum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mmax\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2900\u001b[39m \u001b[43m                          \u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkeepdims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minitial\u001b[49m\u001b[43m=\u001b[49m\u001b[43minitial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m=\u001b[49m\u001b[43mwhere\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\App\\Python1\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:86\u001b[39m, in \u001b[36m_wrapreduction\u001b[39m\u001b[34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[39m\n\u001b[32m     83\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     84\u001b[39m             \u001b[38;5;28;01mreturn\u001b[39;00m reduction(axis=axis, out=out, **passkwargs)\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mufunc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreduce\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mpasskwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# batch_texts = list(islice(dataset_iter, 64*3000))\n",
    "\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(range(0,3000*64))\n",
    "for t in pbar:\n",
    "    \n",
    "    text = batch_texts[t]['text']\n",
    "    \n",
    "    tokens = tokenizer.encode(text).ids\n",
    "    # Không thêm chiều batch sớm\n",
    "    x_tokens = tokens[:-1]\n",
    "    y_tokens = tokens[1:]\n",
    "\n",
    "    # Sau đó mới tạo mảng và reshape thành (1, seq_len)\n",
    "    X = np.array(x_tokens, dtype=np.int32).reshape(1, -1)\n",
    "    y = np.array(y_tokens, dtype=np.int32).reshape(1, -1)\n",
    "\n",
    "    model.fit(X, y, learning_rate=1e-3, epochs=1, batch_size=1 ,verbose=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "bdfe863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anh cứu cứu cứu của các của các cứu, được nghiên cứu, được nghiên cứu, các cứu, được nghiên cứu. Các cứu, được nghiên cứu. Sự. Sự. Sự. Sự. Các cứu. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự,. Sự. Sự. Sự. Sự. Sự. Sự. Sự,. Sự. Các.2,,,,. Các. Sự,. Sự,. Sự,. Sự. Sự. Sự, và. Sự. Sự. Sự. Sự. Sự. Sự.2,.2,.2,,,,.2,,. Sự,. Sự, được trị, được trị, các cứu, pH, pH, pH, pH, cứu cứu, các cứu,,,. Sự,. Sự,. Sự,,,,,,,. Sự,. Sự,. Sự. Sự. Sự. Sự,. Sự. Sự, các cứu,. Sự. Sự. Sự. Sự. Sự. Sự, được trị,. Sự, các. Sự,. Sự,. Sự. Sự. Sự. Sự. Sự. Sự,,,,,,, và các trị,,,,, và các trị, các trị, và các các các độ,. Sự, các. Sự,, các các các các. Sự,,,,,,,.yme, với với với với các với các với. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự,. Sự,,,,,,,,,,,,,,,,. Sự. Sự,. Sự. Sự trị, sự trị. Sự trị. Sự trị. Sự. Sự. Sự. Sự. Sự. Sự. Sự. Sự và. Sự. Sự. Sự. Sự. các.ate.ate. các trị. các. các..... nhân. nhân. nhân,. các.ic,,,,,,,.. độ. độ thể,.. nhân,,..ate,............................ với,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,........ nhân,,,,,,,,,,. Sự,. Sự,. các. các. bào,,,,. Sự. với. các. Sự..... Sự. bào. Các. Các. Sự. Sự. Các. bào,. Các. Sự và sự thể,,. Các tính,. Sự,. Sự,. Sự. Sự. Sự. độ. độ... trị. độ thấy,. độ. độ,,. độ.. độ,. độ. độ.... độ. độ.|\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, max_tokens, tokenizer, model,top_k=1, temperature=1.0):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.k = top_k\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        # self.maxlen = maxlen\n",
    "        pad_token = '<|pad|>'\n",
    "        self.pad_id = tokenizer.token_to_id(pad_token)\n",
    "    def softmax(self, logits):\n",
    "        logits = np.asarray(logits)  # Chuyển về numpy array\n",
    "        return softmax(logits / self.temperature)\n",
    "    \n",
    "    def sample_from(self, logits):\n",
    "        \"\"\"Top-k sampling from logits (1D array)\"\"\"\n",
    "        logits = np.asarray(logits).astype(np.float32)  # Đảm bảo numpy\n",
    "        top_k_indices = np.argpartition(-logits, self.k)[:self.k]\n",
    "        top_k_logits = logits[top_k_indices]\n",
    "        probs = self.softmax(top_k_logits)\n",
    "        \n",
    "        # Chuyển về CPU để dùng numpy.random.choice\n",
    "        probs_cpu = np.asarray(probs)\n",
    "        sampled_pos = np.random.choice(len(top_k_indices), size=1, p=probs_cpu)[0]\n",
    "        return int(np.asarray(top_k_indices[sampled_pos]))  # Chuyển về int\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        start_tokens = self.tokenizer.encode(prompt).ids\n",
    "        token_ids = start_tokens.copy()\n",
    "        \n",
    "        for _ in range(self.max_tokens):\n",
    "            # if len(token_ids) < self.maxlen:\n",
    "            #     input_ids = token_ids + [self.pad_id] * (self.maxlen - len(token_ids))\n",
    "            # else:\n",
    "            #     input_ids = token_ids[-self.maxlen:]\n",
    "            input_ids = token_ids\n",
    "            # Tạo numpy array\n",
    "            input_array = np.array([input_ids], dtype=np.int32)  # (1, seq_len)\n",
    "            logits = self.model(input_array)     # Output shape: (1, seq_len, vocab_size)\n",
    "            # print(logits.shape)\n",
    "            last_pos = len(token_ids) - 1\n",
    "            # if last_pos >= self.maxlen:\n",
    "            #     last_pos = self.maxlen - 1\n",
    "            \n",
    "            next_logits = logits[0,0]   # Shape: (vocab_size,)\n",
    "            next_id = self.sample_from(next_logits)\n",
    "            \n",
    "            token_ids.append(next_id)\n",
    "        \n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "        \n",
    "generator = TextGenerator(\n",
    "    max_tokens=640,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    top_k=1,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "q = 'anh'\n",
    "text = generator.generate(q)\n",
    "print(text+\"|\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
