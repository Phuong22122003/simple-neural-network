{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200b69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\Python1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from models import Model_Adam\n",
    "from layers import Embedding,PositionalEncoding,TransformerBlock,Dense,LayerNorm,softmax\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "36f9f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8a8c197a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\Python1\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\admin\\.cache\\huggingface\\hub\\datasets--vietgpt--wikipedia_vi. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer.from_file(r\"D:\\Workspace\\Python\\Project\\NeuralNetwork\\dataset\\tokenizer\\tokenizer_vi.json\")\n",
    "\n",
    "# dataset = load_dataset(\"justinphan3110/vi_pubmed\", split=\"vi\", streaming=True)\n",
    "dataset = load_dataset(\"vietgpt/wikipedia_vi\", split=\"train\", streaming=True)\n",
    "dataset_iter = iter(islice(dataset, 64*3000))\n",
    "pad_token = '<|pad|>'\n",
    "maxlen = 64\n",
    "pad_id = tokenizer.token_to_id(pad_token)\n",
    "def split_token_batch(texts):\n",
    "        x, y = [], []\n",
    "    \n",
    "        # batch encode\n",
    "        encodings = tokenizer.encode_batch(texts)\n",
    "    \n",
    "        for encoding in encodings:\n",
    "            token = encoding.ids\n",
    "    \n",
    "            if len(token) <= maxlen + 1:\n",
    "                token = token + (maxlen+1-len(token))*[pad_id]\n",
    "                x.append(token[:-1])\n",
    "                y.append(token[1:])\n",
    "                \n",
    "    \n",
    "            else:\n",
    "                x.append(token[:maxlen])\n",
    "                y.append(token[1:maxlen + 1])\n",
    "        return x, y\n",
    "def data_generator(texts):\n",
    "    X, Y = split_token_batch([str(t['text']) for t in texts])\n",
    "    return np.array(X, dtype=np.int32), np.array(Y, dtype=np.int32)\n",
    "\n",
    "\n",
    "# batch_texts = list(islice(dataset_iter, 64*3))\n",
    "# X,y = data_generator(batch_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0659964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(batch_size, seq_len):\n",
    "    \"\"\"Create causal mask for autoregressive generation\"\"\"\n",
    "    mask = np.ones((batch_size, seq_len, seq_len))\n",
    "    return np.tril(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f61afb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class MyGPTModel(Model_Adam):\n",
    "    \"\"\"GPT-style autoregressive transformer model\"\"\"\n",
    "    def __init__(self, vocab_size=10000, max_len=256, embedding_dim=768,\n",
    "                 n_heads=12, n_layers=12):\n",
    "        pad_token = '<|pad|>'\n",
    "        pad_id = tokenizer.token_to_id(pad_token)\n",
    "        super().__init__()\n",
    "        # super().__init__(vocab_size=vocab_size, row_num=1_000_000, maxlen=max_len, pad_id = pad_id)\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.max_len = max_len\n",
    "        self.n_heads = n_heads\n",
    "        self.n_layers = n_layers\n",
    "        self.d_ff = 4 * embedding_dim  \n",
    "        \n",
    "        self.token_embedding = Embedding(vocab_size, embedding_dim)\n",
    "        self.pos_encoding = PositionalEncoding(max_len, embedding_dim)\n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embedding_dim, n_heads, self.d_ff)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        \n",
    "        self.ln_f = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.lm_head = Dense(vocab_size, activation='softmax')\n",
    "        # self.lm_head = Dense_Attention(vocab_size, activation='softmax')\n",
    "    def call(self, input_ids,training=False):\n",
    "        batch_size, seq_len = input_ids.shape       \n",
    "\n",
    "        x = self.token_embedding(input_ids)\n",
    "        x = self.pos_encoding(x,training=False)\n",
    "        \n",
    "        causal_mask = create_causal_mask(batch_size, seq_len)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block.forward(x, mask=causal_mask)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        # logits = self.lm_head(x)\n",
    "        # print(training)\n",
    "        if training:\n",
    "            logits = self.lm_head(x)           # (batch, seq_len, vocab_size)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, -1:, :]) # (batch, vocab_size) - last token\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _all_layer(self):\n",
    "        \"\"\"Override to collect all layers including transformer blocks\"\"\"\n",
    "        layers = []\n",
    "\n",
    "        layers.append(self.token_embedding)\n",
    "        layers.extend(self.transformer_blocks)\n",
    "        layers.append(self.ln_f)\n",
    "        layers.append(self.lm_head)\n",
    "        \n",
    "        return layers\n",
    "\n",
    "model = MyGPTModel(\n",
    "    vocab_size=10000,\n",
    "    embedding_dim=16,      \n",
    "    max_len=64,            \n",
    "    n_heads=4,                 \n",
    "    n_layers=1,\n",
    ")\n",
    "\n",
    "# model.fit(X, y, learning_rate=1e-3, epochs=1, batch_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "621bb5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 27/192000 [00:02<2:28:50, 21.50it/s] "
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# dataset = load_dataset(\"vietgpt/wikipedia_vi\", split=\"train\", streaming=True)\n",
    "\n",
    "\n",
    "for idx, sample in enumerate(tqdm(dataset, total=64*3000)):\n",
    "    if idx >= 64 * 3000:\n",
    "        break  \n",
    "\n",
    "    text = sample[\"text\"]\n",
    "    tokens = tokenizer.encode(text).ids\n",
    "\n",
    "    if len(tokens) < 512 or len(tokens)>1024:\n",
    "        continue\n",
    "    \n",
    "\n",
    "    x_tokens = tokens[:-1]\n",
    "    y_tokens = tokens[1:]\n",
    "\n",
    "    X = np.array(x_tokens, dtype=np.int32).reshape(1, -1)\n",
    "    y = np.array(y_tokens, dtype=np.int32).reshape(1, -1)\n",
    "\n",
    "    model.fit(X, y, learning_rate=1e-3, epochs=1, batch_size=1 ,verbose=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "bdfe863d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anh các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các với các với các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các các với các các các các các các.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " là các các các các các các các các các các các các với các với các với các các các một các hiện là một.\n",
      " là một, là một, là một, là một.\n",
      ", với các với các một.\n",
      ", các các một.\n",
      ".\n",
      ", một một một một một một một một một một các một một một một một các một một một một một một một một một một một một một.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " và một một một một các các các các các các các các một một các các các các các các một và các các các các các các các các các các các các các các các các các các.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " một một một một một một một một một một.\n",
      ".\n",
      " với một với một với với với một.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " và các một.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " là các các các các các các các các các một một một các các các các các một các một các các các các các các các các các các các các các các các các các các các các các các hiện các hiện các các các các các các các các các tính.\n",
      " công.\n",
      " công.\n",
      ", các công.\n",
      " công.\n",
      " ra.\n",
      " ra.\n",
      " quan.\n",
      " quan.\n",
      " quan.\n",
      " quan.\n",
      " quan.\n",
      " ra ra các cứu quan quan quan quan quan quan quan quan.\n",
      " quan.\n",
      " quan.\n",
      " tính.\n",
      ".\n",
      " quan.\n",
      " quan.\n",
      ".\n",
      " tính.\n",
      " quan.\n",
      " quan.\n",
      " quan.\n",
      " quan.\n",
      ".\n",
      " tính.\n",
      " tính.\n",
      " tính.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " quan.\n",
      ".\n",
      " quan.\n",
      ".\n",
      ".\n",
      ".\n",
      " quan.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ",.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      " quan.\n",
      " quan.\n",
      ",.\n",
      ".\n",
      ",.\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      ".\n",
      "|\n"
     ]
    }
   ],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, max_tokens, tokenizer, model,top_k=1, temperature=1.0):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.k = top_k\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "        # self.maxlen = maxlen\n",
    "        pad_token = '<|pad|>'\n",
    "        self.pad_id = tokenizer.token_to_id(pad_token)\n",
    "    def softmax(self, logits):\n",
    "        logits = np.asarray(logits)  # Chuyển về numpy array\n",
    "        return softmax(logits / self.temperature)\n",
    "    \n",
    "    def sample_from(self, logits):\n",
    "        \"\"\"Top-k sampling from logits (1D array)\"\"\"\n",
    "        logits = np.asarray(logits).astype(np.float32)  # Đảm bảo numpy\n",
    "        top_k_indices = np.argpartition(-logits, self.k)[:self.k]\n",
    "        top_k_logits = logits[top_k_indices]\n",
    "        probs = self.softmax(top_k_logits)\n",
    "        \n",
    "        # Chuyển về CPU để dùng numpy.random.choice\n",
    "        probs_cpu = np.asarray(probs)\n",
    "        sampled_pos = np.random.choice(len(top_k_indices), size=1, p=probs_cpu)[0]\n",
    "        return int(np.asarray(top_k_indices[sampled_pos]))  # Chuyển về int\n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        start_tokens = self.tokenizer.encode(prompt).ids\n",
    "        token_ids = start_tokens.copy()\n",
    "        \n",
    "        for _ in range(self.max_tokens):\n",
    "            # if len(token_ids) < self.maxlen:\n",
    "            #     input_ids = token_ids + [self.pad_id] * (self.maxlen - len(token_ids))\n",
    "            # else:\n",
    "            #     input_ids = token_ids[-self.maxlen:]\n",
    "            input_ids = token_ids\n",
    "            # Tạo numpy array\n",
    "            input_array = np.array([input_ids], dtype=np.int32)  # (1, seq_len)\n",
    "            logits = self.model(input_array)     # Output shape: (1, seq_len, vocab_size)\n",
    "            # print(logits.shape)\n",
    "            last_pos = len(token_ids) - 1\n",
    "            # if last_pos >= self.maxlen:\n",
    "            #     last_pos = self.maxlen - 1\n",
    "            \n",
    "            next_logits = logits[0,0]   # Shape: (vocab_size,)\n",
    "            next_id = self.sample_from(next_logits)\n",
    "            \n",
    "            token_ids.append(next_id)\n",
    "        \n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "        \n",
    "generator = TextGenerator(\n",
    "    max_tokens=1024,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    top_k=1,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "q = 'anh'\n",
    "text = generator.generate(q)\n",
    "print(text+\"|\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
