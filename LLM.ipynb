{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36f9f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install tokenizers datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "200b69bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\App\\Python1\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from tokenizers import Tokenizer\n",
    "from models import Model_Adam\n",
    "from layers import Embedding,PositionalEncoding,TransformerBlock,Dense,LayerNorm,softmax\n",
    "import numpy as np\n",
    "from datasets import load_dataset\n",
    "from itertools import islice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659964c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_causal_mask(batch_size, seq_len):\n",
    "    \"\"\"Create causal mask for autoregressive generation\"\"\"\n",
    "    mask = np.ones((batch_size, seq_len, seq_len))\n",
    "    return np.tril(mask)\n",
    "\n",
    "\n",
    "class MyGPTModel(Model_Adam):\n",
    "    \"\"\"GPT-style autoregressive transformer model\"\"\"\n",
    "    def __init__(self, vocab_size=10000, max_len=256, embedding_dim=768,\n",
    "                 n_heads=12, n_layers=12):\n",
    "        super().__init__()\n",
    "        d_ff = 4 * embedding_dim  \n",
    "        self.token_embedding = Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        self.pos_encoding = PositionalEncoding(max_len, embedding_dim)\n",
    "        \n",
    "        self.transformer_blocks = [\n",
    "            TransformerBlock(embedding_dim, n_heads, d_ff)\n",
    "            for _ in range(n_layers)\n",
    "        ]\n",
    "        \n",
    "        self.ln_f = LayerNorm(embedding_dim)\n",
    "\n",
    "        self.lm_head = Dense(vocab_size, activation='softmax')\n",
    "    def call(self, input_ids,training=False):\n",
    "        batch_size, seq_len = input_ids.shape       \n",
    "\n",
    "        x = self.token_embedding(input_ids)\n",
    "        \n",
    "        x = self.pos_encoding(x, training=training)\n",
    "        \n",
    "        causal_mask = create_causal_mask(batch_size, seq_len)\n",
    "        \n",
    "        for block in self.transformer_blocks:\n",
    "            x = block.forward(x, mask=causal_mask)\n",
    "        \n",
    "        x = self.ln_f(x)\n",
    "        \n",
    "        if training:\n",
    "            logits = self.lm_head(x)           # (batch, seq_len, vocab_size)\n",
    "        else:\n",
    "            logits = self.lm_head(x[:, -1:, :]) # (batch, vocab_size) - last token\n",
    "        \n",
    "        return logits\n",
    "    \n",
    "    def _all_layer(self):\n",
    "        \"\"\"Override to collect all layers including transformer blocks\"\"\"\n",
    "        layers = []\n",
    "\n",
    "        layers.append(self.token_embedding)\n",
    "        layers.extend(self.transformer_blocks)\n",
    "        layers.append(self.ln_f)\n",
    "        layers.append(self.lm_head)\n",
    "        \n",
    "        return layers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8a8c197a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer.from_file(r\"D:\\Workspace\\Python\\Project\\NeuralNetwork\\dataset\\tokenizer\\tokenizer_vi.json\")\n",
    "\n",
    "pad_token = '<|pad|>'\n",
    "pad_id = tokenizer.token_to_id(pad_token)\n",
    "\n",
    "def split_token_batch(texts, maxlen):\n",
    "        x, y = [], []\n",
    "    \n",
    "        # batch encode\n",
    "        encodings = tokenizer.encode_batch(texts)\n",
    "    \n",
    "        for encoding in encodings:\n",
    "            token = encoding.ids\n",
    "    \n",
    "            if len(token) <= maxlen + 1:\n",
    "                token = token + (maxlen+1-len(token))*[pad_id]\n",
    "                x.append(token[:-1])\n",
    "                y.append(token[1:])\n",
    "            else:\n",
    "                x.append(token[:maxlen])\n",
    "                y.append(token[1:maxlen + 1])\n",
    "        return x, y\n",
    "def data_generator(texts, maxlen=32):\n",
    "    X, Y = split_token_batch(texts, maxlen)\n",
    "    return np.array(X, dtype=np.int32), np.array(Y, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfe863d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, max_tokens, tokenizer, model,top_k=1, temperature=1.0):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_tokens = max_tokens\n",
    "        self.k = top_k\n",
    "        self.temperature = temperature\n",
    "        self.model = model\n",
    "    def softmax(self, logits):\n",
    "        logits = np.asarray(logits) \n",
    "        return softmax(logits / self.temperature)\n",
    "    \n",
    "    def sample_from(self, logits):\n",
    "        \"\"\"Top-k sampling from logits (1D array)\"\"\"\n",
    "        logits = np.asarray(logits).astype(np.float32)\n",
    "        top_k_indices = np.argpartition(-logits, self.k)[:self.k]\n",
    "        top_k_logits = logits[top_k_indices]\n",
    "        probs = self.softmax(top_k_logits)\n",
    "        \n",
    "        probs_cpu = np.asarray(probs)\n",
    "        sampled_pos = np.random.choice(len(top_k_indices), size=1, p=probs_cpu)[0]\n",
    "        return int(np.asarray(top_k_indices[sampled_pos])) \n",
    "    \n",
    "    def generate(self, prompt):\n",
    "        start_tokens = self.tokenizer.encode(prompt).ids\n",
    "        token_ids = start_tokens.copy()\n",
    "        \n",
    "        for _ in range(self.max_tokens):\n",
    "\n",
    "            input_ids = token_ids\n",
    "            \n",
    "            input_array = np.array([input_ids], dtype=np.int32)  # (1, seq_len)\n",
    "            logits = self.model(input_array)     # Output shape: (1, seq_len, vocab_size)\n",
    "            \n",
    "            next_logits = logits[0,0]   # Shape: (vocab_size,)\n",
    "            next_id = self.sample_from(next_logits)\n",
    "            \n",
    "            token_ids.append(next_id)\n",
    "        \n",
    "        return self.tokenizer.decode(token_ids, skip_special_tokens=False)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "621bb5d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|question|>he is walking<|end_question|><|answer|> ngán ngán ngán tẻtaitai cậu ngôi Mông Mông Mông Mông rơi showbiz showbiz showbiztai ngán ngán rơi� rơi�� Hồ Hồ họa họa họa họa Quyên CSVN|\n"
     ]
    }
   ],
   "source": [
    "MAXLEN = 32\n",
    "EMBEDDING_DIM = 128\n",
    "N_LAYERS = 1\n",
    "VOCAB_SIZE=10000\n",
    "\n",
    "model = MyGPTModel(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embedding_dim=EMBEDDING_DIM,      \n",
    "    max_len=MAXLEN,            \n",
    "    n_heads=4,                 \n",
    "    n_layers=N_LAYERS,\n",
    ")\n",
    "generator = TextGenerator(\n",
    "    max_tokens=MAXLEN,\n",
    "    tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    top_k=1,\n",
    "    temperature=0.9,\n",
    ")\n",
    "\n",
    "q = '<|question|>he is walking<|end_question|><|answer|>'\n",
    "text = generator.generate(q)\n",
    "print(text+\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e703093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset = load_dataset(\"justinphan3110/vi_pubmed\", split=\"vi\", streaming=True)\n",
    "\n",
    "# texts = list(islice(dataset, 64*300))\n",
    "\n",
    "# texts = [str(t['text']) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4deb7106",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(r'dataset\\translation\\spa.txt',encoding='utf8') as f:\n",
    "    lines = f.read().split(\"\\n\")[:-1]\n",
    "texts = []\n",
    "for line in lines:\n",
    "    eng, spa = line.split(\"\\t\")\n",
    "    texts.append(f'<|question|>{eng}<|end_question|><|answer|>{spa}<|end_answer|>')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4c3cb141",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<|question|>Go.<|end_question|><|answer|>Vaya.<|end_answer|>'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7f37fb65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1:  21%|██        | 3079/14871 [14:30<55:32,  3.54batch/s, loss=2.34]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m X,y = data_generator(texts, maxlen=MAXLEN)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mto_one_hot\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m q = \u001b[33m'\u001b[39m\u001b[33mHôm đó\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m      4\u001b[39m text = generator.generate(q)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\Project\\NeuralNetwork\\models\\model_with_adam.py:100\u001b[39m, in \u001b[36mModel_Adam.fit\u001b[39m\u001b[34m(self, inputs, outputs, epochs, batch_size, learning_rate, beta1, beta2, epsilon, verbose, to_one_hot)\u001b[39m\n\u001b[32m     97\u001b[39m         param_id = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mlayer_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_param_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mparam_idx\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     99\u001b[39m         \u001b[38;5;66;03m# Apply Adam update\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m100\u001b[39m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madam_update\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_clipped\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m                       \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    103\u001b[39m avg_loss = total_loss / steps_per_epoch\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m verbose \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(pbar, tqdm):\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Workspace\\Python\\Project\\NeuralNetwork\\models\\model_with_adam.py:56\u001b[39m, in \u001b[36mModel_Adam.adam_update\u001b[39m\u001b[34m(self, param, grad, param_id, learning_rate, beta1, beta2, epsilon)\u001b[39m\n\u001b[32m     53\u001b[39m v_hat = \u001b[38;5;28mself\u001b[39m.v[param_id] / (\u001b[32m1\u001b[39m - beta2 ** \u001b[38;5;28mself\u001b[39m.t)\n\u001b[32m     55\u001b[39m \u001b[38;5;66;03m# Update parameters\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m param[:] -= learning_rate * m_hat / (\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msqrt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mv_hat\u001b[49m\u001b[43m)\u001b[49m + epsilon)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "X,y = data_generator(texts, maxlen=MAXLEN)\n",
    "model.fit(X, y, epochs=2, learning_rate=1e-3, batch_size=8, verbose= True, to_one_hot= True)\n",
    "q = '<|question|>he is walking<|end_question|><|answer|>'\n",
    "text = generator.generate(q)\n",
    "print(text+\"|\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c0adaefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|question|>he<|end_question|><|answer|>He som work.<|end_question|><|answer|>Es es una es.<|end_answer|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|><|pad|>|\n"
     ]
    }
   ],
   "source": [
    "q = '<|question|>he<|end_question|><|answer|>'\n",
    "text = generator.generate(q)\n",
    "print(text+\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size = 1000\n",
    "\n",
    "for i in range(0, len(texts), batch_size):\n",
    "    print(f'===================={i}========================')\n",
    "    batch_texts = texts[i:i + batch_size]\n",
    "    \n",
    "    X, y = data_generator(batch_texts)\n",
    "    \n",
    "    loss = model.fit(X, y, epochs=1, learning_rate=1e-3, batch_size=8, verbose=False)\n",
    "    \n",
    "    prompt = 'hôm đó'\n",
    "    answer = generator.generate(prompt)\n",
    "    print(answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
