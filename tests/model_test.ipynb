{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "f3c3075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "from typing import List\n",
    "\n",
    "def cross_entropy_derivative(y_true, y_pred):\n",
    "    epsilon = 1e-9  # Small value to prevent division by zero\n",
    "    return - (y_true / (y_pred + epsilon)) / y_true.shape[0]\n",
    "\n",
    "def cross_entropy(y_true, y_pred, epsilon=1e-12):\n",
    "    \"\"\"\n",
    "    y_true: (batch_size, num_classes) - one-hot\n",
    "    y_pred: (batch_size, num_classes) - output of softmax\n",
    "    \"\"\"\n",
    "    y_pred = np.clip(y_pred, epsilon, 1. - epsilon) \n",
    "    loss = -np.sum(y_true * np.log(y_pred), axis=1) \n",
    "    return np.mean(loss) \n",
    "def linear(Z):\n",
    "    return Z\n",
    "\n",
    "def linear_derivative(Z):\n",
    "    return np.ones_like(Z)\n",
    "\n",
    "\n",
    "def relu(Z):\n",
    "    return np.maximum(0, Z)\n",
    "\n",
    "\n",
    "def relu_derivative(Z):\n",
    "    return np.where(Z > 0, 1, 0)\n",
    "\n",
    "\n",
    "def leaky_relu(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "\n",
    "def leaky_relu_derivative(Z, alpha=0.01):\n",
    "    return np.where(Z > 0, 1, alpha)\n",
    "\n",
    "\n",
    "def softmax(Z):\n",
    "    expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True)) \n",
    "    return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_backward(x, grad_output):\n",
    "    sig = sigmoid(x)\n",
    "    return grad_output * sig * (1 - sig)\n",
    "\n",
    "def softmax_derivative(s): \n",
    "    \"\"\"\n",
    "    s: shape=(num_class)\n",
    "    \"\"\"\n",
    "    s = s.reshape(-1, 1)  # convert to vector column\n",
    "    return np.diagflat(s) - np.dot(s, s.T)  # Jacobian matrix\n",
    "\n",
    "def softmax_derivative_from_output_2d(softmax_output):\n",
    "    \"\"\"\n",
    "    Calculate Jacobian of Softmax for each sample in batch.\n",
    "    \n",
    "    softmax_output: numpy array (shape: [batch_size, num_classes])\n",
    "    Output: numpy array (shape: [batch_size, num_classes, num_classes])\n",
    "    \"\"\"\n",
    "    batch_size, num_classes = softmax_output.shape\n",
    "    jacobian_matrices = np.zeros((batch_size, num_classes, num_classes))\n",
    "\n",
    "    for i in range(batch_size): \n",
    "        s = softmax_output[i, :].reshape(-1, 1) \n",
    "        jacobian_matrices[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobian_matrices\n",
    "class Layer:\n",
    "    def forward(self,inputs):\n",
    "        raise NotImplementedError()\n",
    "    def backward(self,grad_outputs):\n",
    "        raise NotImplementedError()\n",
    "    def parameters(self):\n",
    "        raise NotImplementedError()\n",
    "\n",
    "class Dense(Layer):\n",
    "    def __init__(self,output_dim,activation = None):\n",
    "        super().__init__()\n",
    "        if activation == 'relu':\n",
    "            self.activation = relu\n",
    "            self.backward_activation = relu_derivative\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = leaky_relu\n",
    "            self.backward_activation = leaky_relu_derivative\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = softmax\n",
    "            self.backward_activation = softmax_derivative_from_output_2d\n",
    "        else:\n",
    "            self.activation = linear\n",
    "            self.backward_activation = linear_derivative\n",
    "            \n",
    "        self.output_dim = output_dim\n",
    "        self.weights = None\n",
    "    def init_weight(self,input_dim):\n",
    "        self.weights = np.random.randn(input_dim,self.output_dim)* np.sqrt(2/input_dim)\n",
    "        self.bias = np.zeros((1,self.output_dim))\n",
    "    def forward(self, inputs):\n",
    "        '''\n",
    "        inputs shape(m,n)\n",
    "        weight shape(n,k)\n",
    "        outputs shape(m,k)\n",
    "        '''\n",
    "\n",
    "        if self.weights is None:\n",
    "            input_dim = inputs.shape[1]\n",
    "            self.init_weight(input_dim=input_dim)\n",
    "        self.inputs = inputs\n",
    "        self.linear_outputs = self.inputs @ self.weights + self.bias\n",
    "        self.outputs = self.activation(self.linear_outputs)\n",
    "        return self.outputs\n",
    "    def __call__(self, inputs):\n",
    "        self.id = int(time.time())\n",
    "        return self.forward(inputs)\n",
    "    def backward(self, grad_outputs):\n",
    "        \n",
    "        if self.activation == softmax:\n",
    "            pass\n",
    "            # # For softmax, we need to handle the Jacobian properly\n",
    "            # da_dz = self.backward_activation(self.outputs)  # (batch_size, output_dim, output_dim)\n",
    "            # dl_dz = np.zeros_like(grad_outputs)  # (batch_size, output_dim)\n",
    "            \n",
    "            # for i in range(batch_size):\n",
    "            #     dl_dz[i, :] = da_dz[i] @ grad_outputs[i, :]\n",
    "            # grad_outputs = dl_dz\n",
    "        else:\n",
    "            # For other activations\n",
    "            grad_outputs = grad_outputs * self.backward_activation(self.linear_outputs)  # (batch_size, output_dim)\n",
    "\n",
    "        # Calculate gradients\n",
    "        self.grad_weights = self.inputs.T @ grad_outputs  # (input_dim, output_dim)\n",
    "        self.grad_bias = np.sum(grad_outputs, axis=0, keepdims=True)  # (1, output_dim)\n",
    "        # Calculate gradient for previous layer\n",
    "        grad_inputs = grad_outputs @ self.weights.T  # (batch_size, input_dim) because dl2 = da1 @ weight => dl2/da  = weight.T\n",
    "        return grad_inputs\n",
    "    \n",
    "    def parameters(self):\n",
    "        return [(self.weights, self.grad_weights), (self.bias, self.grad_bias)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a6a0a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def fit(self, inputs, outputs, epochs = 1, batch_size=32, learning_rate=1e-4, loss=None):\n",
    "        \n",
    "        steps = inputs.shape[0]// batch_size\n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            steps_per_epoch = 0\n",
    "            for i in range(0, inputs.shape[0], batch_size):\n",
    "                inputs_batch = inputs[i:i+batch_size, :] \n",
    "                outputs_batch = outputs[i:i+batch_size, :]   \n",
    "                \n",
    "                # Forward pass\n",
    "                pred_outputs = self.call(inputs_batch)\n",
    "                \n",
    "                loss_val = cross_entropy(outputs_batch, pred_outputs)\n",
    "                total_loss += loss_val\n",
    "                steps_per_epoch += 1\n",
    "                # Backward pass\n",
    "                dl_da = cross_entropy_derivative(outputs_batch, pred_outputs)\n",
    "                layers = self._all_layer()\n",
    "                for layer in reversed(layers):\n",
    "                    dl_da = layer.backward(dl_da)\n",
    "                for layer in layers:\n",
    "                    for param, grad in layer.parameters():\n",
    "                        np.clip(grad, -1, 1, out=grad)\n",
    "                        param[:]-= learning_rate * grad\n",
    " \n",
    "            avg_loss = total_loss / steps\n",
    "            print(f'Epoch: {epoch+1}/{epochs} ==========================> {steps}/{steps} steps. loss {avg_loss}')\n",
    "\n",
    "     \n",
    "    def call(self,inputs):\n",
    "        raise NotImplementedError()\n",
    "    def __call__(self, inputs):\n",
    "        return self.call(inputs)\n",
    "    def _all_layer(self):\n",
    "        layers = []\n",
    "        for attr in self.__dict__.values():\n",
    "            if isinstance(attr,Layer):\n",
    "                layers.append(attr)\n",
    "        layers.sort(key=lambda l:l.id)\n",
    "        ids = [l.id for l in layers]\n",
    "        print(ids)  # để xem ID sau sort\n",
    "        return\n",
    "        return layers  \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "5a55d22f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = Dense(128,'relu')\n",
    "        self.layer2 = Dense(10,'softmax')\n",
    "    def call(self, inputs):\n",
    "        x = self.layer1(inputs)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0ae7ee3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/1 ==========================> 0/0 steps. loss -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\admin\\AppData\\Local\\Temp\\ipykernel_15144\\646996598.py:28: RuntimeWarning: divide by zero encountered in scalar divide\n",
      "  avg_loss = total_loss / steps\n"
     ]
    }
   ],
   "source": [
    "a = MyModel()\n",
    "x = np.random.randn(10,20)\n",
    "y = np.random.randn(10,10)\n",
    "a.fit(x,y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
