{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1894aa29",
   "metadata": {},
   "source": [
    "### Formular: y = 5x-10\n",
    "\n",
    "- target: y = 10 find x. Expected x = 4\n",
    "- loss function: \n",
    "$$\n",
    "z = \\frac{1}{2}(y - y_{\\text{target}})^2\n",
    "$$\n",
    "- dz/dy = y-y_target\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial y} = y - y_{\\text{target}}\n",
    "$$\n",
    "- dy/dx = 5\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial x} = 5\n",
    "$$\n",
    "- dz/dx = dz/dy * dy/dx= (y-y_target)*5\n",
    "$$\n",
    "\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdot \\frac{\\partial y}{\\partial x} = (y - y_{\\text{target}}) \\cdot 5\n",
    "$$\n",
    "\n",
    "- update x: x = x - learning_rate * dz/dx\n",
    "$$\n",
    "x = x - \\eta \\cdot \\frac{\\partial z}{\\partial x}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40f90a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x: 3.999720871150599 y: 9.998604355752995\n"
     ]
    }
   ],
   "source": [
    "x = 0\n",
    "y_target = 10\n",
    "learning_rate = 1e-3 \n",
    "def loss(y,y_target):\n",
    "    return 1/2*(y-y_target)**2\n",
    "for i in range(1000):\n",
    "    y = 5*x-10\n",
    "    x = x - learning_rate * (y-y_target)*5\n",
    "    y = 5*x-10\n",
    "    if abs(loss(y,y_target))<1e-6:\n",
    "        break\n",
    "print(f'x: {x}',f'y: {y}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fe7c24",
   "metadata": {},
   "source": [
    "### Forward pass:\n",
    "\n",
    "Given:\n",
    "\n",
    "$$\n",
    "X = \\begin{bmatrix}\n",
    "x_1 & x_2 \\\\\n",
    "x_3 & x_4\n",
    "\\end{bmatrix}, \\quad\n",
    "W = \\begin{bmatrix}\n",
    "w_1 & w_2 \\\\\n",
    "w_3 & w_4\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "Output:\n",
    "\n",
    "$$\n",
    "y = \\begin{bmatrix}\n",
    "y_1 & y_2\n",
    "\\end{bmatrix}, \\quad\n",
    "y_{\\text{target}} = \\begin{bmatrix}\n",
    "y_1' & y_2'\n",
    "\\end{bmatrix}\n",
    "\n",
    "$$\n",
    "\n",
    "With:\n",
    "\n",
    "$$\n",
    "y_1 = x_1 w_1 + x_2 w_3 \\\\\n",
    "y_2 = x_1 w_2 + x_2 w_4\n",
    "\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Loss function:\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\frac{1}{2} (y - y_{\\text{target}})^2\n",
    "\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient of the loss w.r.t. output:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_1} = y_1 - y_1' \\\\\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial y_2} = y_2 - y_2'\n",
    "\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Derivatives of output w.r.t. weights:\n",
    "\n",
    "From:\n",
    "\n",
    "$$\n",
    "y_1 = x_1 w_1 + x_2 w_3 \\Rightarrow\n",
    "\\frac{\\partial y_1}{\\partial w_1} = x_1, \\quad\n",
    "\\frac{\\partial y_1}{\\partial w_3} = x_2\n",
    "\n",
    "$$\n",
    "\n",
    "$$\n",
    "y_2 = x_1 w_2 + x_2 w_4 \\Rightarrow\n",
    "\\frac{\\partial y_2}{\\partial w_2} = x_1, \\quad\n",
    "\\frac{\\partial y_2}{\\partial w_4} = x_2\n",
    "\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Gradient of the loss w.r.t. weights:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = X^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial y}\n",
    "\n",
    "$$\n",
    "\n",
    "Dimensions:\n",
    "\n",
    "So:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} \\in \\mathbb{R}^{2 \\times 2}\n",
    "\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Multiple samples (batch training):\n",
    "\n",
    "If we train on multiple samples:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial w_1} =\n",
    "\\sum_{i=1}^n \\left( \\frac{\\partial \\mathcal{L}^{(i)}}{\\partial y_1^{(i)}} \\cdot \\frac{\\partial y_1^{(i)}}{\\partial w_1} \\right)\n",
    "\n",
    "$$\n",
    "\n",
    "Which generalizes to:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\mathcal{L}}{\\partial W} = X^\\top \\cdot \\frac{\\partial \\mathcal{L}}{\\partial y}\n",
    "\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### Weight update rule:\n",
    "\n",
    "$$\n",
    "W = W - \\eta \\cdot \\frac{\\partial \\mathcal{L}}{\\partial W}\n",
    "\n",
    "$$\n",
    "\n",
    "Where $\\eta$  is the **learning rate**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "0a1579d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a98aa13c",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr = np.array([\n",
    "    [-5,6,-5,2],\n",
    "    [1,2,9,-5],\n",
    "    [-5,-0.5,-5,2],\n",
    "    [-5,6,-5,2],\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5a830b5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2., 6., 2., 2.],\n",
       "       [2., 2., 9., 2.],\n",
       "       [2., 2., 2., 2.],\n",
       "       [2., 6., 2., 2.]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "np.maximum(2,arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "02be278d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 1],\n",
       "       [1, 1, 1, 0],\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 1, 0, 1]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(arr>0,1,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d10f7a72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "403.1778962208906"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2.718**6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "38d38cf4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6],\n",
       "       [7]])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum([[1,2,3],[1,2,4]],axis=-1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "9e9efb18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.],\n",
       "        [9.],\n",
       "        [2.],\n",
       "        [6.]]),\n",
       " array([6., 9., 2., 6.]))"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(arr, axis=-1, keepdims=True),np.max(arr, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a183ccba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[6.73794700e-03, 4.03428793e+02, 6.73794700e-03, 7.38905610e+00],\n",
       "        [2.71828183e+00, 7.38905610e+00, 8.10308393e+03, 6.73794700e-03],\n",
       "        [6.73794700e-03, 6.06530660e-01, 6.73794700e-03, 7.38905610e+00],\n",
       "        [6.73794700e-03, 4.03428793e+02, 6.73794700e-03, 7.38905610e+00]]),\n",
       " array([[4.10831325e+02],\n",
       "        [8.11319800e+03],\n",
       "        [8.00906265e+00],\n",
       "        [4.10831325e+02]]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(arr),np.sum(np.exp(arr),axis=-1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "6b7e7ea9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.64007625e-05, 9.81981578e-01, 1.64007625e-05, 1.79856200e-02],\n",
       "       [3.35044433e-04, 9.10745195e-04, 9.98753380e-01, 8.30492119e-07],\n",
       "       [8.41290335e-04, 7.57305425e-02, 8.41290335e-04, 9.22586877e-01],\n",
       "       [1.64007625e-05, 9.81981578e-01, 1.64007625e-05, 1.79856200e-02]])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(arr)/np.sum(np.exp(arr),axis=-1,keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1cfcef46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 3, 3)"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def softmax_derivative_from_output_2d(softmax_output):\n",
    "    \"\"\"\n",
    "    Calculate Jacobian of Softmax for each column in batch.\n",
    "\n",
    "    softmax_output: numpy array (shape: [num_classes, batch_size])\n",
    "    Output: numpy array (shape: [batch_size, num_classes, num_classes])\n",
    "    \"\"\"\n",
    "\n",
    "    num_classes, batch_size = softmax_output.shape\n",
    "    jacobian_matrices = np.zeros((batch_size, num_classes, num_classes))\n",
    "\n",
    "    for i in range(batch_size): \n",
    "        s = softmax_output[:, i].reshape(-1, 1) \n",
    "        jacobian_matrices[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "    return jacobian_matrices\n",
    "softmax_derivative_from_output_2d(np.array([\n",
    "    [1,2,3],\n",
    "    [1,2,3],\n",
    "    [1,2,3],\n",
    "])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "98e21fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "========epoch: 1||loss: 0.7233======================\n",
      "========epoch: 2||loss: 0.3278======================\n",
      "========epoch: 3||loss: 0.2727======================\n",
      "========epoch: 4||loss: 0.2401======================\n",
      "========epoch: 5||loss: 0.2166======================\n",
      "========epoch: 6||loss: 0.1982======================\n",
      "========epoch: 7||loss: 0.1831======================\n",
      "========epoch: 8||loss: 0.1704======================\n",
      "========epoch: 9||loss: 0.1594======================\n",
      "========epoch: 10||loss: 0.1497======================\n",
      "Accuracy: 0.9546\n",
      "Predictions shape: (10000,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "class Dense:\n",
    "    @staticmethod\n",
    "    def linear(Z):\n",
    "        return Z\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_derivative(Z):\n",
    "        return np.ones_like(Z)\n",
    "    \n",
    "    @staticmethod\n",
    "    def relu(Z):\n",
    "        return np.maximum(0, Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def relu_derivative(Z):\n",
    "        return np.where(Z > 0, 1, 0)\n",
    "    \n",
    "    @staticmethod\n",
    "    def leaky_relu(Z, alpha=0.01):\n",
    "        return np.where(Z > 0, Z, alpha * Z)\n",
    "\n",
    "    @staticmethod\n",
    "    def leaky_relu_derivative(Z, alpha=0.01):\n",
    "        return np.where(Z > 0, 1, alpha)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax(Z):\n",
    "        expZ = np.exp(Z - np.max(Z, axis=1, keepdims=True)) \n",
    "        return expZ / np.sum(expZ, axis=1, keepdims=True)\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivative(s): \n",
    "        \"\"\"\n",
    "        s: shape=(num_class)\n",
    "        \"\"\"\n",
    "        s = s.reshape(-1, 1)  # convert to vector column\n",
    "        return np.diagflat(s) - np.dot(s, s.T)  # Jacobian matrix\n",
    "    \n",
    "    @staticmethod\n",
    "    def softmax_derivative_from_output_2d(softmax_output):\n",
    "        \"\"\"\n",
    "        Calculate Jacobian of Softmax for each sample in batch.\n",
    "        \n",
    "        softmax_output: numpy array (shape: [batch_size, num_classes])\n",
    "        Output: numpy array (shape: [batch_size, num_classes, num_classes])\n",
    "        \"\"\"\n",
    "        batch_size, num_classes = softmax_output.shape\n",
    "        jacobian_matrices = np.zeros((batch_size, num_classes, num_classes))\n",
    "\n",
    "        for i in range(batch_size): \n",
    "            s = softmax_output[i, :].reshape(-1, 1) \n",
    "            jacobian_matrices[i] = np.diagflat(s) - np.dot(s, s.T)\n",
    "\n",
    "        return jacobian_matrices\n",
    "    \n",
    "    def __init__(self, output_size, activation='relu', learning_rate=1e-4):\n",
    "        self.output_size = output_size\n",
    "        self.learning_rate = learning_rate\n",
    "        self.is_output = False\n",
    "        self.weight = None\n",
    "        self.bias = None\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = self.relu\n",
    "            self.backward_activation = self.relu_derivative\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = self.leaky_relu\n",
    "            self.backward_activation = self.leaky_relu_derivative\n",
    "        elif activation == 'softmax':\n",
    "            self.activation = self.softmax\n",
    "            self.backward_activation = self.softmax_derivative_from_output_2d\n",
    "        else:\n",
    "            self.activation = self.linear\n",
    "            self.backward_activation = self.linear_derivative\n",
    "\n",
    "    def get_input_size(self):\n",
    "        return self.input_size\n",
    "    \n",
    "    def get_output_size(self):\n",
    "        return self.output_size\n",
    "\n",
    "    def init_weight(self, input_size):\n",
    "        self.input_size = input_size\n",
    "        # Xavier initialization\n",
    "        self.weight = np.random.randn(input_size, self.output_size) * np.sqrt(2/input_size)\n",
    "        self.bias = np.zeros((1, self.output_size))\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        # inputs.shape = (batch_size, input_size)\n",
    "        # weight.shape = (input_size, output_size)\n",
    "        \n",
    "        if self.weight is None:\n",
    "            self.init_weight(inputs.shape[1])\n",
    "        \n",
    "        # Z.shape = (batch_size, output_size)\n",
    "        Z = inputs @ self.weight + self.bias\n",
    "        \n",
    "        # A.shape = (batch_size, output_size)\n",
    "        A = self.activation(Z)\n",
    "        \n",
    "        # saving for backward\n",
    "        self.inputs = inputs \n",
    "        self.Z = Z\n",
    "        self.A = A\n",
    "        \n",
    "        return A\n",
    "    # dl/da da/dz dl/dz * dz/dw  z = x@w\n",
    "    def backward(self, dl_da):\n",
    "        # dl_da.shape = (batch_size, output_size)\n",
    "        batch_size = dl_da.shape[0]\n",
    "        \n",
    "        if self.activation == self.softmax:\n",
    "            # For softmax, we need to handle the Jacobian properly\n",
    "            da_dz = self.backward_activation(self.A)  # (batch_size, output_size, output_size)\n",
    "            dl_dz = np.zeros_like(dl_da)  # (batch_size, output_size)\n",
    "            \n",
    "            for i in range(batch_size):\n",
    "                dl_dz[i, :] = da_dz[i] @ dl_da[i, :]\n",
    "        else:\n",
    "            # For other activations\n",
    "            dl_dz = dl_da * self.backward_activation(self.Z)  # (batch_size, output_size)\n",
    "\n",
    "        # Calculate gradients\n",
    "        dl_dw = self.inputs.T @ dl_dz  # (input_size, output_size)\n",
    "        dl_db = np.sum(dl_dz, axis=0, keepdims=True)  # (1, output_size)\n",
    "        \n",
    "        # Update weights and bias\n",
    "        self.weight = self.weight - self.learning_rate * dl_dw\n",
    "        self.bias = self.bias - self.learning_rate * dl_db\n",
    "        \n",
    "        # Calculate gradient for previous layer\n",
    "        pre_dl_da = dl_dz @ self.weight.T  # (batch_size, input_size) because dl2 = da1 @ weight => dl2/da  =weight.T\n",
    "        \n",
    "        return pre_dl_da\n",
    "\n",
    "class Sequential:\n",
    "    def __init__(self, layers: list):\n",
    "        self.layers: list = layers\n",
    "        self.layers[-1].is_output = True\n",
    "    \n",
    "    @staticmethod\n",
    "    def cross_entropy_derivative(y_true, y_pred):\n",
    "        epsilon = 1e-9  # Small value to prevent division by zero\n",
    "        return - (y_true / (y_pred + epsilon)) / y_true.shape[0]\n",
    "    \n",
    "    def fit(self, data, label, batch_size=10, epochs=2, learning_rate=None):\n",
    "        if learning_rate is not None:\n",
    "            for layer in self.layers:\n",
    "                layer.learning_rate = learning_rate\n",
    "                \n",
    "        for epoch in range(epochs):\n",
    "            total_loss = 0\n",
    "            num_batches = 0\n",
    "            \n",
    "            for i in range(0, data.shape[0], batch_size):\n",
    "                data_batch = data[i:i+batch_size, :]      # shape: (batch_size, 784)\n",
    "                label_batch = label[i:i+batch_size, :]    # shape: (batch_size, 10)\n",
    "                \n",
    "                # Forward pass\n",
    "                A = data_batch\n",
    "                for layer in self.layers:\n",
    "                    A = layer.forward(A)\n",
    "\n",
    "                # Calculate loss\n",
    "                A_clipped = np.clip(A, 1e-9, 1.0) \n",
    "                loss = -np.sum(label_batch * np.log(A_clipped)) / label_batch.shape[0]\n",
    "                total_loss += loss\n",
    "                num_batches += 1\n",
    "                \n",
    "                # Backward pass\n",
    "                dl_da = self.cross_entropy_derivative(label_batch, A)\n",
    "                for layer in reversed(self.layers):\n",
    "                    dl_da = layer.backward(dl_da)\n",
    "\n",
    "            avg_loss = total_loss / num_batches\n",
    "            print(f'========epoch: {epoch+1}||loss: {avg_loss:.4f}======================')\n",
    "                \n",
    "    def predict(self, test_data):\n",
    "        A = test_data\n",
    "        for layer in self.layers:\n",
    "            A = layer.forward(A)\n",
    "\n",
    "        return np.argmax(A, axis=1)  \n",
    "\n",
    "if __name__ == '__main__':\n",
    "    (X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "    X_train = X_train.reshape(-1, np.prod(X_train.shape[1:])) / 255.0  # Normalize, shape (60000, 784)\n",
    "    X_test = X_test.reshape(-1, np.prod(X_test.shape[1:])) / 255.0    # Normalize, shape (10000, 784)\n",
    "    y_train = np.squeeze(y_train)\n",
    "    y_test = np.squeeze(y_test)\n",
    "    \n",
    "    Y_train_one_hot = np.eye(10)[y_train]  # Shape (60000, 10)\n",
    "    Y_test_one_hot = np.eye(10)[y_test]    # Shape (10000, 10)\n",
    "\n",
    "    hidden_size = 256\n",
    "    output_size = 10\n",
    "    batch_size = 64\n",
    "    epochs = 10\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    model = Sequential([\n",
    "        Dense(hidden_size, activation='relu'),\n",
    "        Dense(256, activation='relu'),\n",
    "        Dense(output_size, activation='softmax') \n",
    "    ])\n",
    "    \n",
    "    model.fit(X_train, Y_train_one_hot, batch_size=batch_size, epochs=epochs, learning_rate=learning_rate)\n",
    "    predictions = model.predict(X_test)\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    accuracy = np.mean(predictions == y_test)\n",
    "    print(f'Accuracy: {accuracy:.4f}')\n",
    "    print(f'Predictions shape: {predictions.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b022ebee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 6\n",
      "0 0\n",
      "5 5\n",
      "4 4\n",
      "9 9\n",
      "9 9\n",
      "2 2\n",
      "1 1\n",
      "9 9\n",
      "4 4\n",
      "8 8\n",
      "7 7\n",
      "3 3\n",
      "9 9\n",
      "7 7\n",
      "4 4\n",
      "4 4\n",
      "4 4\n",
      "9 9\n",
      "2 2\n",
      "5 5\n",
      "4 4\n",
      "7 7\n",
      "6 6\n",
      "4 7\n",
      "9 9\n",
      "0 0\n",
      "5 5\n",
      "8 8\n",
      "5 5\n",
      "6 6\n",
      "6 6\n",
      "5 5\n",
      "7 7\n",
      "8 8\n",
      "1 1\n",
      "0 0\n",
      "1 1\n",
      "6 6\n",
      "4 4\n",
      "6 6\n",
      "7 7\n",
      "3 3\n",
      "1 1\n",
      "7 7\n",
      "1 1\n",
      "8 8\n",
      "2 2\n",
      "0 0\n",
      "9 2\n",
      "9 9\n",
      "9 9\n",
      "5 5\n",
      "5 5\n",
      "1 1\n",
      "5 5\n",
      "6 6\n",
      "0 0\n",
      "3 3\n",
      "4 4\n",
      "4 4\n",
      "6 6\n",
      "5 5\n",
      "4 4\n",
      "6 6\n",
      "5 5\n",
      "4 4\n",
      "5 5\n",
      "1 1\n",
      "4 4\n",
      "4 4\n",
      "7 7\n",
      "2 2\n",
      "3 3\n",
      "2 2\n",
      "7 7\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "8 8\n",
      "1 1\n",
      "8 8\n",
      "5 5\n",
      "0 0\n",
      "8 8\n",
      "9 9\n",
      "2 2\n",
      "5 5\n",
      "0 0\n",
      "1 1\n",
      "1 1\n",
      "1 1\n",
      "0 0\n",
      "3 9\n",
      "0 0\n",
      "3 3\n",
      "1 1\n",
      "6 6\n",
      "4 4\n",
      "2 2\n"
     ]
    }
   ],
   "source": [
    "for i in range(100,200):\n",
    "    print(predictions[i],y_test[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ac44725",
   "metadata": {},
   "source": [
    "y= ax + b\n",
    "dy/dx = a\n",
    "z1 = x@w1 + b\n",
    "\n",
    "dz1/dw1 = x.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4ef451",
   "metadata": {},
   "source": [
    "[1,2,3,5.7,8,9,2,0,1,5,5,2] index 10\n",
    "[xin chào xin chào] = [3,5,3,5]\n",
    "embeding = [\n",
    "    [0.1,0.2,0.3] 1\n",
    "    [0.1,0.2,0.3] 2\n",
    "    [0.1,0.2,0.3] 3\n",
    "    [0.1,0.2,0.3] 4\n",
    "    [0.1,0.2,0.3] 5\n",
    "    [0.1,0.2,0.3] 6\n",
    "    [0.1,0.2,0.3] 7\n",
    "    [0.1,0.2,0.3] 8 \n",
    "    [0.1,0.2,0.3] 9\n",
    "    [0.1,0.2,0.3] 10\n",
    "]\n",
    "\n",
    "embeding = x = \n",
    "[ \n",
    "    [0.1,0.2,0.3]\n",
    "    [0.1,0.2,0.3]\n",
    "    [0.1,0.2,0.3]\n",
    "    [0.1,0.2,0.3]\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d2bbad0",
   "metadata": {},
   "source": [
    "y\n",
    "\n",
    "x\n",
    "\n",
    "w1\n",
    "\n",
    "z1 = x@w1 + b\n",
    "\n",
    "dl/dz1\n",
    "\n",
    "dz1/dx\n",
    "\n",
    "dz1/dx = w1.T\n",
    "\n",
    "dl/dx = dl/dz1 * dz1/dx \n",
    "\n",
    "a1 = activation(z1)\n",
    "\n",
    "w2\n",
    "\n",
    "z2 = a1@w2 + b\n",
    "\n",
    "a2 = activation(z2)\n",
    "\n",
    "loss = l(a2,y) = 1/2(a2-y)^2\n",
    "\n",
    "d1= dl/a2\n",
    "d2 = da2/dz2\n",
    "d3 = dz2/dw2\n",
    "\n",
    "dl/dw2 = d1*d2*d3\n",
    "\n",
    "w2 = w2 - leaning_rate*dl/dw2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
